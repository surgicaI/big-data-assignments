SUMMARY: The Google File System
By, Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung

The paper explains Google File System which is a scalable distributed file system. GFS is optimized for large distributed data-intensive applications. Paper describes the underlying architecture and how the observations of application workloads and technological environment at google led to the design of this file system. GFS supports all the standard operations on files such as create, delete, open, close, read, and write which are supported by all other file systems. GFS offers few additional operations like, snapshot which creates a copy of a file or directory tree at low cost and record append which allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity of each individual client’s append. There are few assumtions kept in mind while designing GFS, like the the system is build from commodity components and will often fail, the system will store modest number of large files and should be efficient for reading and writing to these large files. A GFS cluster consists of a single master server and several chunkservers. Files are divided into fixed sized chunks of 64MB by the master with unique 64-bit handle and then stored on chunkservers as regular linux files. Usually every chunk is replicated onto three chunkservers but number of replicas can be configured by the user. The master maintains all file system metadata which includes the namespace, access control information, the mapping from files to chunks, and the current locations of chunks. Master also periodically communicate with chunkservers and collects its state. All data bearing operations go directly to chunkservers and not pass through master because that will create bottleneck in master. Because the size of files is large thus caching is not done, it also eliminates cache coherence issues. The client sends a request with filename and chunk index to master and gets the chunk handle and location of replicas. The client then sends request to one of the replicas(preferably the closest one) with chunk handle and no further client-master interaction is needed for any further read from the same file. Since applications mostly read and write large files sequentially, thus client typically asks for multiple chunks in the same request which sidesteps several future client-master interactions at practically no extra cost hence large chunk size reduces the workload. Upon startup, the master builds up its memory data structures by asking the chunkservers which chunks they contain and their location. Master also records mutations to this metadata in operational log which is written to disc. This log system (operational log) is the only persistent record of metadata and serves as a logical time line that defines the order of concurrent operations. Using a log allows us to update the master state simply, reliably, and without risking inconsistencies in the event of a master crash.

BY: 
Simranjyot Singh Gill
