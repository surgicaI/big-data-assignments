Summary of MapReduce: Simplified Data Processing on Large Clusters
By, Jeffrey Dean and Sanjay Ghemawat

The paper presents MapReduce programming model, which process a large amount of data in parallel over distributed systems for high-performance computing. MapReduce consists of several machines within a cluster with mainly one master(maybe more for backup) and remaining workers. These workers are divided into Mappers and Reducers. The master is responsible for evenly dividing, and assigning the task to workers. The progress and failures is monitored by the master, by periodically pinging these machines. The Map invocations are distributed across multiple machines by automatically partitioning the input data into a set of M splits. A map function(written by the user) is provided to the mapper, which takes in a key and value pair as input and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key "I" in "key/list of values" format. These results are written back to the disc. Reduce invocations are distributed by partitioning the intermediate key space into R pieces using a partitioning function (e.g: hash(key) mod R ). The number of partitions "R" and the partitioning function are specified by the user. Each partition's address is then passed to the reduce workers (Reducer) by the Master. All the values associated with a particular key ends up at same reducer becuase the data is sorted on intermidiate keys before its passed to reducers. At the Reducer, the reduce function(written by the user) merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per Reduce invocation. The output is written to a separate global output file. The paper also discusse some refinements that can be made to MapReduce. Default partition function use hash value of key for partitioning, but user can provide a special partition function which can be useful in certain situations. The intermediate key/value pairs are processed in increasing key order which makes it easy to generate a sorted output file per partition. In another suggested refinement, an optional intermidiate combiner function can be provided by the user for partial merging of data before it reaches the Reducer. Implementing a reader interface to support a new input type is also suggested as a refinement. Another refinement can be providing an optional mode of execution where the MapReduce library detects which records cause deterministic crashes and skips these records in order to make forward progress. Refinements like local execution, status information and counters are also discussed giving us more insight into optimizing MapReduce. In simple words, MapReduce is an abstraction for parallelization, load balancing, data distribution and fault-tolerance.

Submitted by:
Simranjyot Singh Gill
